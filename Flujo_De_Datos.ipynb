{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6644f9ca-a281-4ea8-afc7-559680a77b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTACIÓN DE LIBRERÍAS NECESARIAS PARA EL PROYECTO\n",
    "\n",
    "import sqlite3\n",
    "import pyodbc\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "import couchdb\n",
    "import json\n",
    "import csv\n",
    "from textblob import TextBlob\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "from bson import ObjectId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f29449c4-2e57-44a0-85a4-7a09381861e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASO DE ARCHIVOS (DE JSON A CSV)\n",
    "\n",
    "with open('BBC_NOTICIAS.json', 'r', encoding='utf-8') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "with open(\"BBC_NOTICIAS.csv\", \"w\", newline='', encoding='utf-8') as csv_file:\n",
    "    # Crear el escritor de CSV\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=data[0].keys())\n",
    "    \n",
    "    # Escribe la cabecera (nombres de columnas)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Escribe los datos\n",
    "    for row in data:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b9f51b1-5be4-4a2c-b851-b6740f9397d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('VentasYPopularidadDeAutosE.json', 'r', encoding='utf-8') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "with open(\"VentasYPopularidadDeAutosE.csv\", \"w\", newline='', encoding='utf-8') as csv_file:\n",
    "    # Crear el escritor de CSV\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=data[0].keys())\n",
    "    \n",
    "    # Escribe la cabecera (nombres de columnas)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Escribe los datos\n",
    "    for row in data:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "530eebc4-3eb7-4503-b8d3-c148ae42e7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Peli.json', 'r', encoding='utf-8') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "with open(\"Peli.csv\", \"w\", newline='', encoding='utf-8') as csv_file:\n",
    "    # Crear el escritor de CSV\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=data[0].keys())\n",
    "    \n",
    "    # Escribe la cabecera (nombres de columnas)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Escribe los datos\n",
    "    for row in data:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a367d964-764f-47e2-94c2-1893962f12f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Restaurantes_ranking.json', 'r', encoding='utf-8') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "with open(\"Restaurantes_ranking.csv\", \"w\", newline='', encoding='utf-8') as csv_file:\n",
    "    # Crear el escritor de CSV\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=data[0].keys())\n",
    "    \n",
    "    # Escribe la cabecera (nombres de columnas)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Escribe los datos\n",
    "    for row in data:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72b62b40-e2f3-4638-a508-0f25e821a1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Olimpicos.json', 'r', encoding='utf-8') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "with open(\"Olimpicos.csv\", \"w\", newline='', encoding='utf-8') as csv_file:\n",
    "    # Crear el escritor de CSV\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=data[0].keys())\n",
    "    \n",
    "    # Escribe la cabecera (nombres de columnas)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Escribe los datos\n",
    "    for row in data:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c83a245a-64db-43c8-b021-1ba00cb292dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('JuegosOlimpicosDATASETS.json', 'r', encoding='utf-8') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "with open(\"Juegos_Olimpicos.csv\", \"w\", newline='', encoding='utf-8') as csv_file:\n",
    "    # Crear el escritor de CSV\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=data[0].keys())\n",
    "    \n",
    "    # Escribe la cabecera (nombres de columnas)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Escribe los datos\n",
    "    for row in data:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ac7e1aa0-cfaf-444b-b46c-e0590d449e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Futbol.json', 'r', encoding='utf-8') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "with open(\"Futbol.csv\", \"w\", newline='', encoding='utf-8') as csv_file:\n",
    "    # Crear el escritor de CSV\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=data[0].keys())\n",
    "    \n",
    "    # Escribe la cabecera (nombres de columnas)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Escribe los datos\n",
    "    for row in data:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6f573165-1dd4-4523-a82d-76c9208430a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('RecetasComida.json', 'r', encoding='utf-8') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "with open(\"RecetasComida.csv\", \"w\", newline='', encoding='utf-8') as csv_file:\n",
    "    # Crear el escritor de CSV\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=data[0].keys())\n",
    "    \n",
    "    # Escribe la cabecera (nombres de columnas)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Escribe los datos\n",
    "    for row in data:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "33570341-b2e1-4cba-89fc-88dc82cb08ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('India_Noticias.json', 'r', encoding='utf-8') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "with open(\"India_Noticias.csv\", \"w\", newline='', encoding='utf-8') as csv_file:\n",
    "    # Crear el escritor de CSV\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=data[0].keys())\n",
    "    \n",
    "    # Escribe la cabecera (nombres de columnas)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Escribe los datos\n",
    "    for row in data:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e477e953-b7b4-49e1-8ae5-9b91076a35c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Mundo_Noticias.json', 'r', encoding='utf-8') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "with open(\"Mundo_Noticias.csv\", \"w\", newline='', encoding='utf-8') as csv_file:\n",
    "    # Crear el escritor de CSV\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=data[0].keys())\n",
    "    \n",
    "    # Escribe la cabecera (nombres de columnas)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Escribe los datos\n",
    "    for row in data:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "af6aa5ce-7a27-4356-a605-954e50c9aad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Crypto_Noticias.json', 'r', encoding='utf-8') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "with open(\"Crypto_Noticias.csv\", \"w\", newline='', encoding='utf-8') as csv_file:\n",
    "    # Crear el escritor de CSV\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=data[0].keys())\n",
    "    \n",
    "    # Escribe la cabecera (nombres de columnas)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Escribe los datos\n",
    "    for row in data:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9b1cedf-ac44-4d7e-a0aa-7750db0c7d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04139fb8-10a6-40ec-9ab8-1d7de44ce642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASO DE ARCHIVOS (DE CSV A JSON)\n",
    "\n",
    "with open(\"BBC_NOTICIAS.csv\", \"r\", encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    data = [row for row in csv_reader]\n",
    "\n",
    "with open(\"BBC_NOTI_JSON.json\", \"w\", encoding='utf-8') as file:\n",
    "    json.dump(data, file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "908266bf-e757-4633-939b-fa47fb452acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Peli.csv\", \"r\", encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    data = [row for row in csv_reader]\n",
    "\n",
    "with open(\"Peli-JSON.json\", \"w\", encoding='utf-8') as file:\n",
    "    json.dump(data, file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f7ce887-524c-483f-858a-f8000a5123df",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Olimpicos.csv\", \"r\", encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    data = [row for row in csv_reader]\n",
    "\n",
    "with open(\"Olimpicos-JSON.json\", \"w\", encoding='utf-8') as file:\n",
    "    json.dump(data, file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac1164e9-251b-428b-94f7-7da0097c5649",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"VentasYPopularidadDeAutosE.csv\", \"r\", encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    data = [row for row in csv_reader]\n",
    "\n",
    "with open(\"VentasYPopularidadDeAutosE-JSON.json\", \"w\", encoding='utf-8') as file:\n",
    "    json.dump(data, file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13c7ac2a-c10e-443b-8ecf-3cde1bdd7ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Restaurantes_ranking.csv\", \"r\", encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    data = [row for row in csv_reader]\n",
    "\n",
    "with open(\"Restaurantes_ranking-JSON.json\", \"w\", encoding='utf-8') as file:\n",
    "    json.dump(data, file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9ce1a05a-db7e-4a97-81ff-16573521fc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Futbol.csv\", \"r\", encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    data = [row for row in csv_reader]\n",
    "\n",
    "with open(\"Futbol_JSON.json\", \"w\", encoding='utf-8') as file:\n",
    "    json.dump(data, file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e40717ea-a4af-4825-a0e8-08dd7969f8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"RecetasComida.csv\", \"r\", encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    data = [row for row in csv_reader]\n",
    "\n",
    "with open(\"RecetasComida_JSON.json\", \"w\", encoding='utf-8') as file:\n",
    "    json.dump(data, file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b0da26b8-cc00-4696-86e4-0b373a31aabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"India_Noticias.csv\", \"r\", encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    data = [row for row in csv_reader]\n",
    "\n",
    "with open(\"India_Noticias_JSON.json\", \"w\", encoding='utf-8') as file:\n",
    "    json.dump(data, file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9c803332-fac1-4981-96c3-807549b986f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Mundo_Noticias.csv\", \"r\", encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    data = [row for row in csv_reader]\n",
    "\n",
    "with open(\"Mundo_Noticias_JSON.json\", \"w\", encoding='utf-8') as file:\n",
    "    json.dump(data, file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "38c2c230-bcae-4ba3-8e4c-d6c83bc8173a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Crypto_Noticias.csv\", \"r\", encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    data = [row for row in csv_reader]\n",
    "\n",
    "with open(\"Crypto_Noticias_JSON.json\", \"w\", encoding='utf-8') as file:\n",
    "    json.dump(data, file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a20936f-d88b-4366-bb37-a9b1e01184bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- LIMPIEZA DE DATOS ---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1cc6bf72-3b9a-42ab-8aef-3989a3a14411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_id            0\n",
      "title          0\n",
      "pubDate        0\n",
      "guid           0\n",
      "link           0\n",
      "description    0\n",
      "dtype: int64\n",
      "El archivo no tiene valores nulos.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('BBC_NOTICIAS.csv')\n",
    "\n",
    "nulos = df.isnull().sum()\n",
    "\n",
    "print(nulos)\n",
    "\n",
    "if nulos.sum() > 0:\n",
    "    print(\"El archivo tiene valores nulos.\")\n",
    "else:\n",
    "    print(\"El archivo no tiene valores nulos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "03211b19-8469-4b5d-bfd6-b490605ef47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_id                  0\n",
      "Unnamed: 1           0\n",
      "id                   0\n",
      "original_title       0\n",
      "original_language    0\n",
      "release_date         0\n",
      "popularity           0\n",
      "vote_average         0\n",
      "vote_count           0\n",
      "media_type           0\n",
      "adult                0\n",
      "dtype: int64\n",
      "El archivo no tiene valores nulos.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Peli.csv')\n",
    "\n",
    "nulos = df.isnull().sum()\n",
    "\n",
    "print(nulos)\n",
    "\n",
    "if nulos.sum() > 0:\n",
    "    print(\"El archivo tiene valores nulos.\")\n",
    "else:\n",
    "    print(\"El archivo no tiene valores nulos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "40eb7291-0780-4ff0-81fe-e552471b7d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_id       0\n",
      "ID        0\n",
      "Name      0\n",
      "Sex       0\n",
      "Age       0\n",
      "Height    0\n",
      "Weight    0\n",
      "Team      0\n",
      "NOC       0\n",
      "Games     0\n",
      "Year      0\n",
      "Season    0\n",
      "City      0\n",
      "Sport     0\n",
      "Event     0\n",
      "Medal     0\n",
      "dtype: int64\n",
      "El archivo no tiene valores nulos.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Olimpicos.csv')\n",
    "\n",
    "nulos = df.isnull().sum()\n",
    "\n",
    "print(nulos)\n",
    "\n",
    "if nulos.sum() > 0:\n",
    "    print(\"El archivo tiene valores nulos.\")\n",
    "else:\n",
    "    print(\"El archivo no tiene valores nulos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e9105891-9241-47d9-910d-a2321d6ef238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_id           0\n",
      "region        0\n",
      "category      0\n",
      "parameter     0\n",
      "mode          0\n",
      "powertrain    0\n",
      "year          0\n",
      "unit          0\n",
      "value         0\n",
      "dtype: int64\n",
      "El archivo no tiene valores nulos.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('VentasYPopularidadDeAutosE.csv')\n",
    "\n",
    "nulos = df.isnull().sum()\n",
    "\n",
    "print(nulos)\n",
    "\n",
    "if nulos.sum() > 0:\n",
    "    print(\"El archivo tiene valores nulos.\")\n",
    "else:\n",
    "    print(\"El archivo no tiene valores nulos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8889bf35-bb7a-4cd1-8a2c-87b12692c45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_id                     0\n",
      "Restaurant ID           0\n",
      "Restaurant Name         0\n",
      "Country Code            0\n",
      "City                    0\n",
      "Address                 0\n",
      "Locality                0\n",
      "Locality Verbose        0\n",
      "Longitude               0\n",
      "Latitude                0\n",
      "Cuisines                0\n",
      "Average Cost for two    0\n",
      "Currency                0\n",
      "Has Table booking       0\n",
      "Has Online delivery     0\n",
      "Is delivering now       0\n",
      "Switch to order menu    0\n",
      "Price range             0\n",
      "Aggregate rating        0\n",
      "Rating color            0\n",
      "Rating text             0\n",
      "Votes                   0\n",
      "dtype: int64\n",
      "El archivo no tiene valores nulos.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Restaurantes_ranking.csv')\n",
    "\n",
    "nulos = df.isnull().sum()\n",
    "\n",
    "print(nulos)\n",
    "\n",
    "if nulos.sum() > 0:\n",
    "    print(\"El archivo tiene valores nulos.\")\n",
    "else:\n",
    "    print(\"El archivo no tiene valores nulos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "840dbfe4-668a-4ce9-a093-990b7eedda2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad575588-3420-43b3-8720-f6c32f41eee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASO DE ARCHIVOS (DE JSON A DB (SQLite))\n",
    "\n",
    "with open (\"BBC_NOTICIAS.json\", \"r\", encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "conexion = sqlite3.connect(\"BBC_NOTICIAS.db\")\n",
    "cursor = conexion.cursor()\n",
    "cursor.execute(\"PRAGMA journal_mode=WAL;\")\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS noticias (\n",
    "    id TEXT PRIMARY KEY,\n",
    "    title TEXT NOT NULL,\n",
    "    pubDate TEXT NOT NULL,\n",
    "    guid TEXT NOT NULL,\n",
    "    link TEXT NOT NULL,\n",
    "    description TEXT NOT NULL\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Insertar datos en la tabla\n",
    "for item in data:\n",
    "    cursor.execute(\"\"\"\n",
    "    INSERT OR IGNORE  INTO noticias (id, title, pubDate, guid, link, description)\n",
    "    VALUES (?, ?, ?, ?, ?, ?)\n",
    "    \"\"\", (\n",
    "        item[\"_id\"][\"$oid\"],  # Convertir el campo _id.$oid a texto\n",
    "        item[\"title\"],\n",
    "        item[\"pubDate\"],\n",
    "        item[\"guid\"],\n",
    "        item[\"link\"],\n",
    "        item[\"description\"]\n",
    "    ))\n",
    "\n",
    "# Guardar los cambios y cerrar la conexión\n",
    "conexion.commit()\n",
    "conexion.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "accf9c1c-babb-4d64-84c9-33ba41291f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"Olimpicos.json\", \"r\", encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "conexion = sqlite3.connect(\"Olimpicos.db\")\n",
    "cursor = conexion.cursor()\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS athletes (\n",
    "    id TEXT PRIMARY KEY,\n",
    "    I_D INTEGER NOT NULL,\n",
    "    Name TEXT NOT NULL,\n",
    "    Sex TEXT NOT NULL,\n",
    "    Age INTEGER,\n",
    "    Height INTEGER,\n",
    "    Weight INTEGER,\n",
    "    Team TEXT NOT NULL,\n",
    "    NOC TEXT NOT NULL,\n",
    "    Games TEXT NOT NULL,\n",
    "    Year INTEGER NOT NULL,\n",
    "    Season TEXT NOT NULL,\n",
    "    City TEXT NOT NULL,\n",
    "    Sport TEXT NOT NULL,\n",
    "    Event TEXT NOT NULL,\n",
    "    Medal TEXT\n",
    ")\"\"\")\n",
    "\n",
    "# Insertar datos en la tabla\n",
    "for item in data:\n",
    "    try:\n",
    "        # Convertir campos como \"Height\" y \"Weight\" de \"NA\" a None si es necesario\n",
    "        height = None if item[\"Height\"] == \"NA\" else item[\"Height\"]\n",
    "        weight = None if item[\"Weight\"] == \"NA\" else item[\"Weight\"]\n",
    "        \n",
    "        # Insertar los datos\n",
    "        cursor.execute(\"\"\"\n",
    "        INSERT OR IGNORE INTO athletes (\n",
    "            id, I_D, Name, Sex, Age, Height, Weight, Team, NOC, Games, Year, Season, City, Sport, Event, Medal\n",
    "        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        \"\"\", (\n",
    "            item[\"_id\"][\"$oid\"],  \n",
    "            item[\"ID\"],           \n",
    "            item[\"Name\"],         \n",
    "            item[\"Sex\"],          \n",
    "            item[\"Age\"],          \n",
    "            height,               \n",
    "            weight,               \n",
    "            item[\"Team\"],         \n",
    "            item[\"NOC\"],          \n",
    "            item[\"Games\"],        \n",
    "            item[\"Year\"],         \n",
    "            item[\"Season\"],       \n",
    "            item[\"City\"],         \n",
    "            item[\"Sport\"],        \n",
    "            item[\"Event\"],        \n",
    "            item[\"Medal\"]\n",
    "        ))\n",
    "\n",
    "    except KeyError as e:\n",
    "        print(f\"Error con el campo {e} en el registro: {item}\")\n",
    "\n",
    "# Guardar los cambios y cerrar la conexión\n",
    "conexion.commit()\n",
    "conexion.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32bd6aec-6ae9-4367-98f8-be8a24041fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"Restaurantes_ranking.json\", \"r\", encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "conexion = sqlite3.connect(\"Restaurantes_ranking.db\")\n",
    "cursor = conexion.cursor()\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS restaurants (\n",
    "    id TEXT PRIMARY KEY,\n",
    "    restaurant_id INTEGER,\n",
    "    restaurant_name TEXT,\n",
    "    country_code INTEGER,\n",
    "    city TEXT,\n",
    "    address TEXT,\n",
    "    locality TEXT,\n",
    "    locality_verbose TEXT,\n",
    "    longitude REAL,\n",
    "    latitude REAL,\n",
    "    cuisines TEXT,\n",
    "    average_cost_for_two INTEGER,\n",
    "    currency TEXT,\n",
    "    has_table_booking TEXT,\n",
    "    has_online_delivery TEXT,\n",
    "    is_delivering_now TEXT,\n",
    "    switch_to_order_menu TEXT,\n",
    "    price_range INTEGER,\n",
    "    aggregate_rating REAL,\n",
    "    rating_color TEXT,\n",
    "    rating_text TEXT,\n",
    "    votes INTEGER\n",
    ")\n",
    "''')\n",
    "\n",
    "# Insertar datos en la tabla\n",
    "\n",
    "for restaurant in data:\n",
    "    # Usar un valor predeterminado si la clave 'Cuisines' no existe\n",
    "    cuisines = restaurant.get('Cuisines', '')\n",
    "    \n",
    "    cursor.execute('''\n",
    "    INSERT INTO restaurants (\n",
    "        id, restaurant_id, restaurant_name, country_code, city, address, locality, locality_verbose,\n",
    "        longitude, latitude, cuisines, average_cost_for_two, currency, has_table_booking,\n",
    "        has_online_delivery, is_delivering_now, switch_to_order_menu, price_range, aggregate_rating,\n",
    "        rating_color, rating_text, votes\n",
    "    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    ''', (\n",
    "        restaurant['_id']['$oid'], restaurant['Restaurant ID'], restaurant['Restaurant Name'], restaurant['Country Code'], restaurant['City'],\n",
    "        restaurant['Address'], restaurant['Locality'], restaurant['Locality Verbose'], restaurant['Longitude'],\n",
    "        restaurant['Latitude'], cuisines, restaurant['Average Cost for two'], restaurant['Currency'],\n",
    "        restaurant['Has Table booking'], restaurant['Has Online delivery'], restaurant['Is delivering now'],\n",
    "        restaurant['Switch to order menu'], restaurant['Price range'], restaurant['Aggregate rating'],\n",
    "        restaurant['Rating color'], restaurant['Rating text'], restaurant['Votes']\n",
    "    ))\n",
    "\n",
    "# Guardar los cambios y cerrar la conexión\n",
    "conexion.commit()\n",
    "conexion.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96263504-bf2f-4755-aabd-32f4e769aa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el archivo JSON\n",
    "with open(\"VentasYPopularidadDeAutosE.json\", \"r\", encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Conectar a la base de datos SQLite (la creará si no existe)\n",
    "conexion = sqlite3.connect(\"VentasPopularidadDeAutosE.db\")\n",
    "cursor = conexion.cursor()\n",
    "\n",
    "# Crear la tabla si no existe\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS historical_data (\n",
    "    id TEXT PRIMARY KEY,\n",
    "    region TEXT,\n",
    "    category TEXT,\n",
    "    parameter TEXT,\n",
    "    mode TEXT,\n",
    "    powertrain TEXT,\n",
    "    year INTEGER,\n",
    "    unit TEXT,\n",
    "    value REAL\n",
    ")\n",
    "''')\n",
    "\n",
    "# Insertar los datos en la tabla\n",
    "for entry in data:\n",
    "    cursor.execute('''\n",
    "    INSERT INTO historical_data (\n",
    "        id, region, category, parameter, mode, powertrain, year, unit, value\n",
    "    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    ''', (\n",
    "        entry['_id']['$oid'], entry['region'], entry['category'], entry['parameter'],\n",
    "        entry['mode'], entry['powertrain'], entry['year'], entry['unit'], entry['value']\n",
    "    ))\n",
    "\n",
    "# Guardar los cambios y cerrar la conexión\n",
    "conexion.commit()\n",
    "conexion.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff398cdf-e52f-4971-8cfc-1771650d7056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el archivo JSON\n",
    "with open(\"PELI.json\", \"r\", encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Conectar a la base de datos SQLite (la creará si no existe)\n",
    "conexion = sqlite3.connect(\"movies.db\")\n",
    "cursor = conexion.cursor()\n",
    "\n",
    "# Crear la tabla si no existe, incluyendo el campo '_id' y el campo con clave vacía\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS movies (\n",
    "    _id TEXT PRIMARY KEY,\n",
    "    empty_key INTEGER,  -- Este es el campo con la clave vacía\n",
    "    id INTEGER,\n",
    "    original_title TEXT,\n",
    "    original_language TEXT,\n",
    "    release_date TEXT,\n",
    "    popularity REAL,\n",
    "    vote_average REAL,\n",
    "    vote_count INTEGER,\n",
    "    media_type TEXT,\n",
    "    adult BOOLEAN\n",
    ")\n",
    "''')\n",
    "\n",
    "# Insertar los datos en la tabla, incluyendo el campo '_id' y el campo con clave vacía\n",
    "for entry in data:\n",
    "    cursor.execute('''\n",
    "    INSERT INTO movies (\n",
    "        _id, empty_key, id, original_title, original_language, release_date, popularity, vote_average,\n",
    "        vote_count, media_type, adult\n",
    "    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    ''', (\n",
    "        entry['_id']['$oid'], entry.get('', None), entry.get('id', None), entry.get('original_title', None),\n",
    "        entry.get('original_language', None), entry.get('release_date', {}).get('$date', None),\n",
    "        entry.get('popularity', None), entry.get('vote_average', None), entry.get('vote_count', None),\n",
    "        entry.get('media_type', None), entry.get('adult', None)\n",
    "    ))\n",
    "\n",
    "# Guardar los cambios y cerrar la conexión\n",
    "conexion.commit()\n",
    "conexion.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99eb685a-a263-43af-89e2-16b58ce17a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------- DE SQL A NoSQL ------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d5cdf71-900e-437b-9519-b8b9ff931c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASO DE ARCHIVOS (DB - SQLITE A MONGODB)\n",
    "\n",
    "sqlite_conn = sqlite3.connect(\"BBC_NOTICIAS.db\") \n",
    "sqlite_cursor = sqlite_conn.cursor()\n",
    "\n",
    "client = MongoClient(\"mongodb://localhost:27017/\") \n",
    "mongodb = client[\"RELACIONAL-NO-RELACIONAL\"]  \n",
    "coleccion = mongodb[\"BBC_NOTICIAS_MONGO\"]\n",
    "sqlite_cursor.execute(\"SELECT * FROM noticias\")\n",
    "columnas = [desc[0] for desc in sqlite_cursor.description]\n",
    "\n",
    "documentos = []\n",
    "for fila in sqlite_cursor.fetchall():\n",
    "    documento = {columnas[i]: fila[i] for i in range(len(columnas))}\n",
    "    documentos.append(documento)\n",
    "\n",
    "if documentos:  # Insertar solo si hay datos\n",
    "    coleccion.insert_many(documentos)\n",
    "\n",
    "# Cerrar conexiones\n",
    "sqlite_conn.close()\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e181421-92c3-4a5d-87d2-1acd455496dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlite_conn = sqlite3.connect(\"Olimpicos.db\") \n",
    "sqlite_cursor = sqlite_conn.cursor()\n",
    "\n",
    "client = MongoClient(\"mongodb://localhost:27017/\") \n",
    "mongodb = client[\"RELACIONAL-NO-RELACIONAL\"]  \n",
    "coleccion = mongodb[\"OLIMPICOS_MONGO\"]\n",
    "sqlite_cursor.execute(\"SELECT * FROM athletes\")\n",
    "columnas = [desc[0] for desc in sqlite_cursor.description]\n",
    "\n",
    "documentos = []\n",
    "for fila in sqlite_cursor.fetchall():\n",
    "    documento = {columnas[i]: fila[i] for i in range(len(columnas))}\n",
    "    documentos.append(documento)\n",
    "\n",
    "if documentos:  # Insertar solo si hay datos\n",
    "    coleccion.insert_many(documentos)\n",
    "\n",
    "# Cerrar conexiones\n",
    "sqlite_conn.close()\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "802b04fd-c2cb-4b37-b1b3-c9abe6199863",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlite_conn = sqlite3.connect(\"Restaurantes_ranking.db\") \n",
    "sqlite_cursor = sqlite_conn.cursor()\n",
    "\n",
    "client = MongoClient(\"mongodb://localhost:27017/\") \n",
    "mongodb = client[\"RELACIONAL-NO-RELACIONAL\"]  \n",
    "coleccion = mongodb[\"RESTAURANTES_RANKING_MONGO\"]\n",
    "sqlite_cursor.execute(\"SELECT * FROM restaurants\")\n",
    "columnas = [desc[0] for desc in sqlite_cursor.description]\n",
    "\n",
    "documentos = []\n",
    "for fila in sqlite_cursor.fetchall():\n",
    "    documento = {columnas[i]: fila[i] for i in range(len(columnas))}\n",
    "    documentos.append(documento)\n",
    "\n",
    "if documentos:  # Insertar solo si hay datos\n",
    "    coleccion.insert_many(documentos)\n",
    "\n",
    "# Cerrar conexiones\n",
    "sqlite_conn.close()\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80c22c82-6ea6-4b5e-9de0-9289d4b701a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlite_conn = sqlite3.connect(\"movies.db\") \n",
    "sqlite_cursor = sqlite_conn.cursor()\n",
    "\n",
    "client = MongoClient(\"mongodb://localhost:27017/\") \n",
    "mongodb = client[\"RELACIONAL-NO-RELACIONAL\"]  \n",
    "coleccion = mongodb[\"PELICULAS_MONGO\"]\n",
    "sqlite_cursor.execute(\"SELECT * FROM movies\")\n",
    "columnas = [desc[0] for desc in sqlite_cursor.description]\n",
    "\n",
    "documentos = []\n",
    "for fila in sqlite_cursor.fetchall():\n",
    "    documento = {columnas[i]: fila[i] for i in range(len(columnas))}\n",
    "    documentos.append(documento)\n",
    "\n",
    "if documentos:  # Insertar solo si hay datos\n",
    "    coleccion.insert_many(documentos)\n",
    "\n",
    "# Cerrar conexiones\n",
    "sqlite_conn.close()\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a764ed04-58d7-4a2c-8854-718516e52541",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlite_conn = sqlite3.connect(\"VentasPopularidadDeAutosE.db\") \n",
    "sqlite_cursor = sqlite_conn.cursor()\n",
    "\n",
    "client = MongoClient(\"mongodb://localhost:27017/\") \n",
    "mongodb = client[\"RELACIONAL-NO-RELACIONAL\"]  \n",
    "coleccion = mongodb[\"VENTA_POPULARIDAD_AUTOS_MONGO\"]\n",
    "sqlite_cursor.execute(\"SELECT * FROM historical_data\")\n",
    "columnas = [desc[0] for desc in sqlite_cursor.description]\n",
    "\n",
    "documentos = []\n",
    "for fila in sqlite_cursor.fetchall():\n",
    "    documento = {columnas[i]: fila[i] for i in range(len(columnas))}\n",
    "    documentos.append(documento)\n",
    "\n",
    "if documentos:  # Insertar solo si hay datos\n",
    "    coleccion.insert_many(documentos)\n",
    "\n",
    "# Cerrar conexiones\n",
    "sqlite_conn.close()\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d87a7b55-2ceb-4e78-bc15-478213fafcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9c922e3-fd57-47d1-a857-e516dae6020b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASO DE ARCHIVOS (DB - SQLITE A COUCHDB)\n",
    "\n",
    "sqlite_conn = sqlite3.connect(\"BBC_NOTICIAS.db\")\n",
    "sqlite_cursor = sqlite_conn.cursor()\n",
    "\n",
    "couch = couchdb.Server(\"http://localhost:5984/\")  \n",
    "usuario = \"admin\" \n",
    "contrasena = \"123456\" \n",
    "couch.resource.credentials = (usuario, contrasena)\n",
    "\n",
    "db_nombre = \"bbc_noticias_couchdb\"\n",
    "if db_nombre in couch:\n",
    "    db = couch[db_nombre]\n",
    "else:\n",
    "    db = couch.create(db_nombre)\n",
    "\n",
    "sqlite_cursor.execute(\"SELECT * FROM noticias\") \n",
    "columnas = [desc[0] for desc in sqlite_cursor.description]\n",
    "\n",
    "for fila in sqlite_cursor.fetchall():\n",
    "    documento = {columnas[i]: fila[i] for i in range(len(columnas))}\n",
    "    db.save(documento)\n",
    "\n",
    "# Cerrar conexiones\n",
    "sqlite_conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0c805f3a-6791-42c8-800b-824b73d43c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlite_conn = sqlite3.connect(\"movies.db\")\n",
    "sqlite_cursor = sqlite_conn.cursor()\n",
    "\n",
    "couch = couchdb.Server(\"http://localhost:5984/\")  \n",
    "usuario = \"admin\" \n",
    "contrasena = \"123456\" \n",
    "couch.resource.credentials = (usuario, contrasena)\n",
    "\n",
    "db_nombre = \"peliculas_couchdb\"\n",
    "if db_nombre in couch:\n",
    "    db = couch[db_nombre]\n",
    "else:\n",
    "    db = couch.create(db_nombre)\n",
    "\n",
    "sqlite_cursor.execute(\"SELECT * FROM movies\") \n",
    "columnas = [desc[0] for desc in sqlite_cursor.description]\n",
    "\n",
    "for fila in sqlite_cursor.fetchall():\n",
    "    documento = {columnas[i]: fila[i] for i in range(len(columnas))}\n",
    "    db.save(documento)\n",
    "\n",
    "# Cerrar conexiones\n",
    "sqlite_conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bc542a43-c9c0-48a1-9ac5-aff71a139e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlite_conn = sqlite3.connect(\"Olimpicos.db\")\n",
    "sqlite_cursor = sqlite_conn.cursor()\n",
    "\n",
    "couch = couchdb.Server(\"http://localhost:5984/\")  \n",
    "usuario = \"admin\" \n",
    "contrasena = \"123456\" \n",
    "couch.resource.credentials = (usuario, contrasena)\n",
    "\n",
    "db_nombre = \"olimpicos_couchdb\"\n",
    "if db_nombre in couch:\n",
    "    db = couch[db_nombre]\n",
    "else:\n",
    "    db = couch.create(db_nombre)\n",
    "\n",
    "sqlite_cursor.execute(\"SELECT * FROM athletes\") \n",
    "columnas = [desc[0] for desc in sqlite_cursor.description]\n",
    "\n",
    "for fila in sqlite_cursor.fetchall():\n",
    "    documento = {columnas[i]: fila[i] for i in range(len(columnas))}\n",
    "    db.save(documento)\n",
    "\n",
    "# Cerrar conexiones\n",
    "sqlite_conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "89fb1826-d742-43ff-ae51-9189d6277dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlite_conn = sqlite3.connect(\"Restaurantes_ranking.db\")\n",
    "sqlite_cursor = sqlite_conn.cursor()\n",
    "\n",
    "couch = couchdb.Server(\"http://localhost:5984/\")  \n",
    "usuario = \"admin\" \n",
    "contrasena = \"123456\" \n",
    "couch.resource.credentials = (usuario, contrasena)\n",
    "\n",
    "db_nombre = \"restaurantes_couchdb\"\n",
    "if db_nombre in couch:\n",
    "    db = couch[db_nombre]\n",
    "else:\n",
    "    db = couch.create(db_nombre)\n",
    "\n",
    "sqlite_cursor.execute(\"SELECT * FROM restaurants\") \n",
    "columnas = [desc[0] for desc in sqlite_cursor.description]\n",
    "\n",
    "for fila in sqlite_cursor.fetchall():\n",
    "    documento = {columnas[i]: fila[i] for i in range(len(columnas))}\n",
    "    db.save(documento)\n",
    "\n",
    "# Cerrar conexiones\n",
    "sqlite_conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "56b764fb-b0be-4ba9-b2ce-9b4af09ba14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlite_conn = sqlite3.connect(\"VentasPopularidadDeAutosE.db\")\n",
    "sqlite_cursor = sqlite_conn.cursor()\n",
    "\n",
    "couch = couchdb.Server(\"http://localhost:5984/\")  \n",
    "usuario = \"admin\" \n",
    "contrasena = \"123456\" \n",
    "couch.resource.credentials = (usuario, contrasena)\n",
    "\n",
    "db_nombre = \"autos_couchdb\"\n",
    "if db_nombre in couch:\n",
    "    db = couch[db_nombre]\n",
    "else:\n",
    "    db = couch.create(db_nombre)\n",
    "\n",
    "sqlite_cursor.execute(\"SELECT * FROM historical_data\") \n",
    "columnas = [desc[0] for desc in sqlite_cursor.description]\n",
    "\n",
    "for fila in sqlite_cursor.fetchall():\n",
    "    documento = {columnas[i]: fila[i] for i in range(len(columnas))}\n",
    "    db.save(documento)\n",
    "\n",
    "# Cerrar conexiones\n",
    "sqlite_conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8dd7662-ae53-4f26-bd03-f0c58b466598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------- DE NoSQL A SQL ---------------------------------------\n",
    "# PASO DE ARCHIVOS (MONGODB A SQL SERVER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "228f05cd-e48c-4717-a3a8-e1fe2228e0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient(\"mongodb://localhost:27017/\")  \n",
    "db = client[\"RELACIONAL-NO-RELACIONAL\"]\n",
    "collection = db[\"BBC_NOTICIAS_MONGO\"]\n",
    "\n",
    "conn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};'\n",
    "                      'SERVER=DESKTOP-CPD7SBF\\\\SQLEXPRESS;'\n",
    "                      'DATABASE=De_NoSQL_SQL;'\n",
    "                      'trusted_connection=yes')\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "IF NOT EXISTS (SELECT * FROM sysobjects WHERE name='noticias' AND xtype='U')\n",
    "CREATE TABLE noticias (\n",
    "    id NVARCHAR(255) PRIMARY KEY,\n",
    "    title NVARCHAR(255),\n",
    "    pubDate DATETIME,\n",
    "    guid NVARCHAR(255),\n",
    "    link NVARCHAR(255),\n",
    "    description TEXT\n",
    ")\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "def insertar_lote(lote):\n",
    "    try:\n",
    "        for item in lote:\n",
    "            cursor.execute(\"\"\"\n",
    "            MERGE INTO noticias AS target\n",
    "            USING (VALUES (?, ?, ?, ?, ?, ?)) AS source (id, title, pubDate, guid, link, description)\n",
    "                ON target.id = source.id\n",
    "            WHEN NOT MATCHED THEN\n",
    "                INSERT (id, title, pubDate, guid, link, description)\n",
    "                VALUES (source.id, source.title, source.pubDate, source.guid, source.link, source.description);\n",
    "            \"\"\", item[0], *item[1:])\n",
    "        \n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        print(f\"Error al insertar lote: {e}\")\n",
    "        conn.rollback() \n",
    "\n",
    "batch_size = 100000  \n",
    "lote = []\n",
    "\n",
    "for document in collection.find({}, no_cursor_timeout=True).batch_size(batch_size):\n",
    "    try:\n",
    "        pubDate = document[\"pubDate\"]\n",
    "        pubDate = datetime.strptime(pubDate, \"%a, %d %b %Y %H:%M:%S GMT\")\n",
    "\n",
    "        lote.append((\n",
    "            document[\"id\"],\n",
    "            document[\"title\"],\n",
    "            pubDate,\n",
    "            document[\"guid\"],\n",
    "            document[\"link\"],\n",
    "            document[\"description\"]\n",
    "        ))\n",
    "        if len(lote) >= batch_size:\n",
    "            insertar_lote(lote)\n",
    "            lote = [] \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error al procesar el documento con id {document.get('id', 'desconocido')}: {e}\")\n",
    "\n",
    "if lote:\n",
    "    insertar_lote(lote)\n",
    "\n",
    "# Cerrar las conexiones\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a9d7848-7272-4e72-812f-1c4e41c46ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient(\"mongodb://localhost:27017/\")  \n",
    "db = client[\"RELACIONAL-NO-RELACIONAL\"]\n",
    "collection = db[\"OLIMPICOS_MONGO\"]\n",
    "\n",
    "conn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};'\n",
    "                      'SERVER=DESKTOP-CPD7SBF\\\\SQLEXPRESS;'\n",
    "                      'DATABASE=De_NoSQL_SQL;'\n",
    "                      'trusted_connection=yes')\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "IF NOT EXISTS (SELECT * FROM sysobjects WHERE name='deportista' AND xtype='U')\n",
    "CREATE TABLE deportistas (\n",
    "    id NVARCHAR(255) PRIMARY KEY,\n",
    "    I_D INT,\n",
    "    Name NVARCHAR(255),\n",
    "    Sex NVARCHAR(50),\n",
    "    Age INT,\n",
    "    Height INT,\n",
    "    Weight INT,\n",
    "    Team NVARCHAR(100),\n",
    "    NOC NVARCHAR(50),\n",
    "    Games NVARCHAR(100),\n",
    "    Year INT,\n",
    "    Season NVARCHAR(50),\n",
    "    City NVARCHAR(100),\n",
    "    Sport NVARCHAR(100),\n",
    "    Event NVARCHAR(255),\n",
    "    Medal NVARCHAR(50)\n",
    ")\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "def insertar_lote(lote):\n",
    "    try:\n",
    "        for item in lote:\n",
    "            cursor.execute(\"\"\"\n",
    "            MERGE INTO deportistas AS target\n",
    "            USING (VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)) AS source (id, I_D, Name, Sex, Age, Height, Weight, Team, NOC, Games, Year, Season, City, Sport, Event, Medal)\n",
    "                ON target.id = source.id\n",
    "            WHEN NOT MATCHED THEN\n",
    "                INSERT (id, I_D, Name, Sex, Age, Height, Weight, Team, NOC, Games, Year, Season, City, Sport, Event, Medal)\n",
    "                VALUES (source.id, source.I_D, source.Name, source.Sex, source.Age, source.Height, source.Weight, source.Team, source.NOC, source.Games, source.Year, source.Season, source.City, source.Sport, source.Event, source.Medal);\n",
    "            \"\"\", item[0], *item[1:])\n",
    "        \n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        print(f\"Error al insertar lote: {e}\")\n",
    "        conn.rollback() \n",
    "\n",
    "batch_size = 100000\n",
    "lote = []\n",
    "\n",
    "for document in collection.find({}, no_cursor_timeout=True).batch_size(batch_size):\n",
    "    try:\n",
    "        def parse_int(value):\n",
    "            try:\n",
    "                return int(value) if value.isdigit() else None\n",
    "            except Exception:\n",
    "                return None\n",
    "\n",
    "        age = parse_int(document.get(\"Age\", \"\"))\n",
    "        year = parse_int(document.get(\"Year\", \"\"))\n",
    "\n",
    "        lote.append((\n",
    "            document[\"id\"],\n",
    "            document[\"I_D\"],\n",
    "            document[\"Name\"],\n",
    "            document[\"Sex\"],\n",
    "            age,\n",
    "            document[\"Height\"],\n",
    "            document[\"Weight\"],\n",
    "            document[\"Team\"],\n",
    "            document[\"NOC\"],\n",
    "            document[\"Games\"],\n",
    "            year,\n",
    "            document[\"Season\"],\n",
    "            document[\"City\"],\n",
    "            document[\"Sport\"],\n",
    "            document[\"Event\"],\n",
    "            document[\"Medal\"]\n",
    "        ))\n",
    "        if len(lote) >= batch_size:\n",
    "            insertar_lote(lote)\n",
    "            lote = []\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error al procesar el documento con id {document.get('id', 'desconocido')}: {e}\")\n",
    "\n",
    "if lote:\n",
    "    insertar_lote(lote)\n",
    "\n",
    "# Cerrar las conexiones\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6c866805-9c4b-40a9-aaaf-98a8b0126454",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient(\"mongodb://localhost:27017/\")  \n",
    "db = client[\"RELACIONAL-NO-RELACIONAL\"]  \n",
    "collection = db[\"PELICULAS_MONGO\"]\n",
    "\n",
    "conn = pyodbc.connect(r'DRIVER={ODBC Driver 17 for SQL Server};'\n",
    "                      r'SERVER=DESKTOP-CPD7SBF\\SQLEXPRESS;'\n",
    "                      r'DATABASE=De_NoSQL_SQL;'\n",
    "                      r'trusted_connection=yes')\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "IF NOT EXISTS (SELECT * FROM sysobjects WHERE name='peliculas' AND xtype='U')\n",
    "CREATE TABLE peliculas (\n",
    "    empty_key INT PRIMARY KEY,  -- Cambiado a empty_key\n",
    "    id INT,\n",
    "    original_title NVARCHAR(255),\n",
    "    original_language NVARCHAR(50),\n",
    "    release_date DATETIME,\n",
    "    popularity FLOAT,\n",
    "    vote_average FLOAT,\n",
    "    vote_count INT,\n",
    "    media_type NVARCHAR(50),\n",
    "    adult BIT\n",
    ")\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "def insertar_lote(lote):\n",
    "    for fila in lote:\n",
    "        try:\n",
    "            cursor.execute(\"\"\"\n",
    "            INSERT INTO peliculas (empty_key, id, original_title, original_language, release_date, popularity, vote_average, vote_count, media_type, adult)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\", fila)\n",
    "        except Exception as e:\n",
    "            print(f\"Error al insertar el registro con empty_key {fila[0]}: {e}\")\n",
    "    conn.commit()\n",
    "\n",
    "batch_size = 7000\n",
    "lote = []\n",
    "\n",
    "for document in collection.find({}, no_cursor_timeout=True).batch_size(batch_size):\n",
    "    release_date = document.get(\"release_date\")\n",
    "    \n",
    "    if release_date:\n",
    "        try:\n",
    "            release_date = datetime.strptime(release_date, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error al convertir la fecha: {e}\")\n",
    "    else:\n",
    "        release_date = None \n",
    "\n",
    "    lote.append((\n",
    "        document[\"empty_key\"],\n",
    "        document[\"id\"],\n",
    "        document[\"original_title\"],\n",
    "        document[\"original_language\"],\n",
    "        release_date,\n",
    "        document[\"popularity\"],\n",
    "        document[\"vote_average\"],\n",
    "        document[\"vote_count\"],\n",
    "        document[\"media_type\"],\n",
    "        bool(document[\"adult\"])\n",
    "    ))\n",
    "\n",
    "    if len(lote) >= batch_size:\n",
    "        insertar_lote(lote)\n",
    "        lote = []\n",
    "\n",
    "if lote:\n",
    "    insertar_lote(lote)\n",
    "\n",
    "# Cerrar las conexiones\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3fe09fc1-da0b-4284-a822-4bad552ef170",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient(\"mongodb://localhost:27017/\")  \n",
    "db = client[\"RELACIONAL-NO-RELACIONAL\"]  \n",
    "collection = db[\"RESTAURANTES_RANKING_MONGO\"]\n",
    "\n",
    "conn = pyodbc.connect(r'DRIVER={ODBC Driver 17 for SQL Server};'\n",
    "                      r'SERVER=DESKTOP-CPD7SBF\\SQLEXPRESS;'\n",
    "                      r'DATABASE=De_NoSQL_SQL;'\n",
    "                      r'trusted_connection=yes')\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "IF NOT EXISTS (SELECT * FROM sysobjects WHERE name='restaurantes' AND xtype='U')\n",
    "CREATE TABLE restaurantes (\n",
    "    id NVARCHAR(50) PRIMARY KEY,\n",
    "    restaurant_id INT,\n",
    "    restaurant_name NVARCHAR(255),\n",
    "    country_code INT,\n",
    "    city NVARCHAR(100),\n",
    "    address NVARCHAR(500),\n",
    "    locality NVARCHAR(255),\n",
    "    locality_verbose NVARCHAR(500),\n",
    "    longitude FLOAT,\n",
    "    latitude FLOAT,\n",
    "    cuisines NVARCHAR(255),\n",
    "    average_cost_for_two INT,\n",
    "    currency NVARCHAR(100),\n",
    "    has_table_booking NVARCHAR(10),\n",
    "    has_online_delivery NVARCHAR(10),\n",
    "    is_delivering_now NVARCHAR(10),\n",
    "    switch_to_order_menu NVARCHAR(10),\n",
    "    price_range INT,\n",
    "    aggregate_rating FLOAT,\n",
    "    rating_color NVARCHAR(50),\n",
    "    rating_text NVARCHAR(50),\n",
    "    votes INT\n",
    ")\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "def insertar_lote(lote):\n",
    "    for fila in lote:\n",
    "        try:\n",
    "            cursor.execute(\"\"\"\n",
    "            INSERT INTO restaurantes (\n",
    "                id, restaurant_id, restaurant_name, country_code, city, address, locality, \n",
    "                locality_verbose, longitude, latitude, cuisines, average_cost_for_two, \n",
    "                currency, has_table_booking, has_online_delivery, is_delivering_now, \n",
    "                switch_to_order_menu, price_range, aggregate_rating, rating_color, \n",
    "                rating_text, votes)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\", fila)\n",
    "        except Exception as e:\n",
    "            print(f\"Error al insertar el registro con id {fila[0]}: {e}\")\n",
    "    conn.commit()\n",
    "\n",
    "batch_size = 7000\n",
    "lote = []\n",
    "\n",
    "for document in collection.find({}, no_cursor_timeout=True).batch_size(batch_size):\n",
    "    lote.append((\n",
    "        document[\"id\"],\n",
    "        document[\"restaurant_id\"],\n",
    "        document[\"restaurant_name\"],\n",
    "        document[\"country_code\"],\n",
    "        document[\"city\"],\n",
    "        document[\"address\"],\n",
    "        document[\"locality\"],\n",
    "        document[\"locality_verbose\"],\n",
    "        document[\"longitude\"],\n",
    "        document[\"latitude\"],\n",
    "        document[\"Cuisines\"],\n",
    "        document[\"average_cost_for_two\"],\n",
    "        document[\"currency\"],\n",
    "        document[\"has_table_booking\"],\n",
    "        document[\"has_online_delivery\"],\n",
    "        document[\"is_delivering_now\"],\n",
    "        document[\"switch_to_order_menu\"],\n",
    "        document[\"price_range\"],\n",
    "        document[\"aggregate_rating\"],\n",
    "        document[\"rating_color\"],\n",
    "        document[\"rating_text\"],\n",
    "        document[\"votes\"]\n",
    "    ))\n",
    "\n",
    "    if len(lote) >= batch_size:\n",
    "        insertar_lote(lote)\n",
    "        lote = []\n",
    "\n",
    "if lote:\n",
    "    insertar_lote(lote)\n",
    "\n",
    "# Cerrar las conexiones\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a78d2bcf-c885-4650-8a64-0e8ad24ff78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient(\"mongodb://localhost:27017/\")  \n",
    "db = client[\"RELACIONAL-NO-RELACIONAL\"]  \n",
    "collection = db[\"VENTA_POPULARIDAD_AUTOS_MONGO\"]\n",
    "\n",
    "conn = pyodbc.connect(r'DRIVER={ODBC Driver 17 for SQL Server};'\n",
    "                      r'SERVER=DESKTOP-CPD7SBF\\SQLEXPRESS;'\n",
    "                      r'DATABASE=De_NoSQL_SQL;'\n",
    "                      r'trusted_connection=yes')\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "IF NOT EXISTS (SELECT * FROM sysobjects WHERE name='electric_vehicles' AND xtype='U')\n",
    "CREATE TABLE electric_vehicles (\n",
    "    id NVARCHAR(50) PRIMARY KEY,\n",
    "    region NVARCHAR(100),\n",
    "    category NVARCHAR(100),\n",
    "    parameter NVARCHAR(100),\n",
    "    mode NVARCHAR(50),\n",
    "    powertrain NVARCHAR(50),\n",
    "    year INT,\n",
    "    unit NVARCHAR(50),\n",
    "    value FLOAT\n",
    ")\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "def insertar_lote(lote):\n",
    "    for fila in lote:\n",
    "        try:\n",
    "            cursor.execute(\"\"\"\n",
    "            INSERT INTO electric_vehicles (\n",
    "                id, region, category, parameter, mode, powertrain, year, unit, value\n",
    "            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\", fila)\n",
    "        except Exception as e:\n",
    "            print(f\"Error al insertar el registro con id {fila[0]}: {e}\")\n",
    "    conn.commit()\n",
    "\n",
    "batch_size = 7000\n",
    "lote = []\n",
    "\n",
    "for document in collection.find({}, no_cursor_timeout=True).batch_size(batch_size):\n",
    "    lote.append((\n",
    "        document[\"id\"],\n",
    "        document.get(\"region\", None),\n",
    "        document.get(\"category\", None),\n",
    "        document.get(\"parameter\", None),\n",
    "        document.get(\"mode\", None),\n",
    "        document.get(\"powertrain\", None),\n",
    "        document.get(\"year\", None),\n",
    "        document.get(\"unit\", None),\n",
    "        document.get(\"value\", None)\n",
    "    ))\n",
    "\n",
    "    if len(lote) >= batch_size:\n",
    "        insertar_lote(lote)\n",
    "        lote = []\n",
    "\n",
    "if lote:\n",
    "    insertar_lote(lote)\n",
    "\n",
    "# Cerrar las conexiones\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d002ad8-7f26-4c34-9d8a-3cede736c1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fbe6aa9-ca05-480b-ac04-0c839be11d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASO DE ARCHIVOS (COUCHDB A SQL SERVER)\n",
    "\n",
    "couch = couchdb.Server(\"http://localhost:5984/\")  \n",
    "usuario = \"admin\" \n",
    "contrasena = \"123456\" \n",
    "couch.resource.credentials = (usuario, contrasena)\n",
    "\n",
    "db = couch['bbc_noticias_couchdb']\n",
    "\n",
    "conn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};'\n",
    "                      'SERVER=DESKTOP-CPD7SBF\\\\SQLEXPRESS;'\n",
    "                      'DATABASE=couchdb_sqlserver;'\n",
    "                      'trusted_connection=yes')\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "IF NOT EXISTS (SELECT * FROM sysobjects WHERE name='noticias' AND xtype='U')\n",
    "CREATE TABLE noticias (\n",
    "    id NVARCHAR(255) PRIMARY KEY,\n",
    "    title NVARCHAR(255),\n",
    "    pubDate DATETIME,\n",
    "    guid NVARCHAR(255),\n",
    "    link NVARCHAR(255),\n",
    "    description TEXT\n",
    ")\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "def insertar_lote(lote):\n",
    "    try:\n",
    "        for item in lote:\n",
    "            cursor.execute(\"\"\"\n",
    "            MERGE INTO noticias AS target\n",
    "            USING (VALUES (?, ?, ?, ?, ?, ?)) AS source (id, title, pubDate, guid, link, description)\n",
    "                ON target.id = source.id\n",
    "            WHEN NOT MATCHED THEN\n",
    "                INSERT (id, title, pubDate, guid, link, description)\n",
    "                VALUES (source.id, source.title, source.pubDate, source.guid, source.link, source.description);\n",
    "            \"\"\", item[0], *item[1:])\n",
    "        \n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        print(f\"Error al insertar lote: {e}\")\n",
    "        conn.rollback() \n",
    "\n",
    "batch_size = 7000  \n",
    "lote = []\n",
    "\n",
    "for doc_id in db:\n",
    "    try:\n",
    "        doc = db[doc_id]\n",
    "        pubDate = doc.get(\"pubDate\")\n",
    "        if pubDate:\n",
    "            pubDate = datetime.strptime(pubDate, \"%a, %d %b %Y %H:%M:%S GMT\")  # Ajustar formato de fecha\n",
    "\n",
    "        lote.append((\n",
    "            doc[\"id\"],                \n",
    "            doc.get(\"title\", None),     \n",
    "            pubDate,                    \n",
    "            doc.get(\"guid\", None),      \n",
    "            doc.get(\"link\", None),      \n",
    "            doc.get(\"description\", None)\n",
    "        ))\n",
    "\n",
    "        if len(lote) >= batch_size:\n",
    "            insertar_lote(lote)\n",
    "            lote = []\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error al procesar el documento con id {doc_id}: {e}\")\n",
    "\n",
    "if lote:\n",
    "    insertar_lote(lote)\n",
    "\n",
    "# Cerrar las conexiones\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eae8f93b-bf79-4dd8-a7b3-0ff0302760b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "couch = couchdb.Server(\"http://localhost:5984/\")  \n",
    "usuario = \"admin\" \n",
    "contrasena = \"123456\" \n",
    "couch.resource.credentials = (usuario, contrasena)\n",
    "\n",
    "db = couch['olimpicos_couchdb']\n",
    "\n",
    "conn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};'\n",
    "                      'SERVER=DESKTOP-CPD7SBF\\\\SQLEXPRESS;'\n",
    "                      'DATABASE=couchdb_sqlserver;'\n",
    "                      'trusted_connection=yes')\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "IF NOT EXISTS (SELECT * FROM sysobjects WHERE name='deportistas' AND xtype='U')\n",
    "CREATE TABLE deportistas (\n",
    "    id NVARCHAR(255) PRIMARY KEY,\n",
    "    I_D INT,\n",
    "    Name NVARCHAR(255),\n",
    "    Sex NVARCHAR(50),\n",
    "    Age INT,\n",
    "    Height INT,\n",
    "    Weight INT,\n",
    "    Team NVARCHAR(100),\n",
    "    NOC NVARCHAR(50),\n",
    "    Games NVARCHAR(100),\n",
    "    Year INT,\n",
    "    Season NVARCHAR(50),\n",
    "    City NVARCHAR(100),\n",
    "    Sport NVARCHAR(100),\n",
    "    Event NVARCHAR(255),\n",
    "    Medal NVARCHAR(50)\n",
    ")\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "def insertar_lote(lote):\n",
    "    try:\n",
    "        for item in lote:\n",
    "            cursor.execute(\"\"\"\n",
    "            MERGE INTO deportistas AS target\n",
    "            USING (VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)) AS source (id, I_D, Name, Sex, Age, Height, Weight, Team, NOC, Games, Year, Season, City, Sport, Event, Medal)\n",
    "                ON target.id = source.id\n",
    "            WHEN NOT MATCHED THEN\n",
    "                INSERT (id, I_D, Name, Sex, Age, Height, Weight, Team, NOC, Games, Year, Season, City, Sport, Event, Medal)\n",
    "                VALUES (source.id, source.I_D, source.Name, source.Sex, source.Age, source.Height, source.Weight, source.Team, source.NOC, source.Games, source.Year, source.Season, source.City, source.Sport, source.Event, source.Medal);\n",
    "            \"\"\", item[0], *item[1:])\n",
    "        \n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        print(f\"Error al insertar lote: {e}\")\n",
    "        conn.rollback() \n",
    "\n",
    "batch_size = 10000\n",
    "lote = []\n",
    "\n",
    "for document_id in db:\n",
    "    try:\n",
    "        document = db[document_id]\n",
    "\n",
    "        def parse_int(value):\n",
    "            try:\n",
    "                return int(value) if str(value).isdigit() else None\n",
    "            except Exception:\n",
    "                return None\n",
    "\n",
    "        age = parse_int(document.get(\"Age\", \"\"))\n",
    "        year = parse_int(document.get(\"Year\", \"\"))\n",
    "\n",
    "        lote.append((\n",
    "            document[\"_id\"],\n",
    "            document.get(\"I_D\", None),\n",
    "            document.get(\"Name\", None),\n",
    "            document.get(\"Sex\", None),\n",
    "            age,\n",
    "            document.get(\"Height\", None),\n",
    "            document.get(\"Weight\", None),\n",
    "            document.get(\"Team\", None),\n",
    "            document.get(\"NOC\", None),\n",
    "            document.get(\"Games\", None),\n",
    "            year,\n",
    "            document.get(\"Season\", None),\n",
    "            document.get(\"City\", None),\n",
    "            document.get(\"Sport\", None),\n",
    "            document.get(\"Event\", None),\n",
    "            document.get(\"Medal\", None)\n",
    "        ))\n",
    "        if len(lote) >= batch_size:\n",
    "            insertar_lote(lote)\n",
    "            lote = []\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error al procesar el documento con id {document_id}: {e}\")\n",
    "\n",
    "if lote:\n",
    "    insertar_lote(lote)\n",
    "\n",
    "# Cerrar las conexiones\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "777b77aa-945d-4c34-a1c9-f079fd600f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "couch = couchdb.Server(\"http://localhost:5984/\")\n",
    "usuario = \"admin\"\n",
    "contrasena = \"123456\"\n",
    "couch.resource.credentials = (usuario, contrasena)\n",
    "\n",
    "db = couch['peliculas_couchdb'] \n",
    "\n",
    "conn = pyodbc.connect(r'DRIVER={ODBC Driver 17 for SQL Server};'\n",
    "                      r'SERVER=DESKTOP-CPD7SBF\\SQLEXPRESS;'\n",
    "                      r'DATABASE=couchdb_sqlserver;'\n",
    "                      r'trusted_connection=yes')\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "IF NOT EXISTS (SELECT * FROM sysobjects WHERE name='peliculas' AND xtype='U')\n",
    "CREATE TABLE peliculas (\n",
    "    empty_key INT PRIMARY KEY,\n",
    "    id INT,\n",
    "    original_title NVARCHAR(255),\n",
    "    original_language NVARCHAR(50),\n",
    "    release_date DATETIME,\n",
    "    popularity FLOAT,\n",
    "    vote_average FLOAT,\n",
    "    vote_count INT,\n",
    "    media_type NVARCHAR(50),\n",
    "    adult BIT\n",
    ")\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "def insertar_lote(lote):\n",
    "    for fila in lote:\n",
    "        try:\n",
    "            cursor.execute(\"\"\"\n",
    "            INSERT INTO peliculas (empty_key, id, original_title, original_language, release_date, popularity, vote_average, vote_count, media_type, adult)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\", fila)\n",
    "        except Exception as e:\n",
    "            print(f\"Error al insertar el registro con empty_key {fila[0]}: {e}\")\n",
    "    conn.commit()\n",
    "\n",
    "batch_size = 7000\n",
    "lote = []\n",
    "\n",
    "for document_id in db:\n",
    "    try:\n",
    "        document = db[document_id]\n",
    "        release_date = document.get(\"release_date\")\n",
    "        if release_date:\n",
    "            try:\n",
    "                release_date = datetime.strptime(release_date, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error al convertir la fecha: {e}\")\n",
    "                release_date = None\n",
    "        else:\n",
    "            release_date = None\n",
    "\n",
    "        lote.append((\n",
    "            document.get(\"empty_key\", None),\n",
    "            document.get(\"id\", None),\n",
    "            document.get(\"original_title\", None),\n",
    "            document.get(\"original_language\", None),\n",
    "            release_date,\n",
    "            document.get(\"popularity\", None),\n",
    "            document.get(\"vote_average\", None),\n",
    "            document.get(\"vote_count\", None),\n",
    "            document.get(\"media_type\", None),\n",
    "            bool(document.get(\"adult\", False))\n",
    "        ))\n",
    "\n",
    "        if len(lote) >= batch_size:\n",
    "            insertar_lote(lote)\n",
    "            lote = []\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error al procesar el documento con id {document_id}: {e}\")\n",
    "\n",
    "if lote:\n",
    "    insertar_lote(lote)\n",
    "\n",
    "# Cerrar las conexiones\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc0bc318-15c5-4697-800a-160f3920bc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "couch = couchdb.Server(\"http://localhost:5984/\")\n",
    "usuario = \"admin\"\n",
    "contrasena = \"123456\"\n",
    "couch.resource.credentials = (usuario, contrasena)\n",
    "\n",
    "db = couch['restaurantes_couchdb'] \n",
    "\n",
    "conn = pyodbc.connect(r'DRIVER={ODBC Driver 17 for SQL Server};'\n",
    "                      r'SERVER=DESKTOP-CPD7SBF\\SQLEXPRESS;'\n",
    "                      r'DATABASE=couchdb_sqlserver;'\n",
    "                      r'trusted_connection=yes')\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "IF NOT EXISTS (SELECT * FROM sysobjects WHERE name='restaurantes' AND xtype='U')\n",
    "CREATE TABLE restaurantes (\n",
    "    id NVARCHAR(50) PRIMARY KEY,\n",
    "    restaurant_id INT,\n",
    "    restaurant_name NVARCHAR(255),\n",
    "    country_code INT,\n",
    "    city NVARCHAR(100),\n",
    "    address NVARCHAR(500),\n",
    "    locality NVARCHAR(255),\n",
    "    locality_verbose NVARCHAR(500),\n",
    "    longitude FLOAT,\n",
    "    latitude FLOAT,\n",
    "    cuisines NVARCHAR(255),\n",
    "    average_cost_for_two INT,\n",
    "    currency NVARCHAR(100),\n",
    "    has_table_booking NVARCHAR(10),\n",
    "    has_online_delivery NVARCHAR(10),\n",
    "    is_delivering_now NVARCHAR(10),\n",
    "    switch_to_order_menu NVARCHAR(10),\n",
    "    price_range INT,\n",
    "    aggregate_rating FLOAT,\n",
    "    rating_color NVARCHAR(50),\n",
    "    rating_text NVARCHAR(50),\n",
    "    votes INT\n",
    ")\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "def insertar_lote(lote):\n",
    "    for fila in lote:\n",
    "        try:\n",
    "            cursor.execute(\"\"\"\n",
    "            INSERT INTO restaurantes (\n",
    "                id, restaurant_id, restaurant_name, country_code, city, address, locality, \n",
    "                locality_verbose, longitude, latitude, cuisines, average_cost_for_two, \n",
    "                currency, has_table_booking, has_online_delivery, is_delivering_now, \n",
    "                switch_to_order_menu, price_range, aggregate_rating, rating_color, \n",
    "                rating_text, votes)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\", fila)\n",
    "        except Exception as e:\n",
    "            print(f\"Error al insertar el registro con id {fila[0]}: {e}\")\n",
    "    conn.commit()\n",
    "\n",
    "batch_size = 7000\n",
    "lote = []\n",
    "\n",
    "for document_id in db:\n",
    "    try:\n",
    "        document = db[document_id]\n",
    "        lote.append((\n",
    "            document.get(\"id\", \"\"),\n",
    "            document.get(\"restaurant_id\", None),\n",
    "            document.get(\"restaurant_name\", \"\"),\n",
    "            document.get(\"country_code\", None),\n",
    "            document.get(\"city\", \"\"),\n",
    "            document.get(\"address\", \"\"),\n",
    "            document.get(\"locality\", \"\"),\n",
    "            document.get(\"locality_verbose\", \"\"),\n",
    "            document.get(\"longitude\", None),\n",
    "            document.get(\"latitude\", None),\n",
    "            document.get(\"Cuisines\", \"\"),\n",
    "            document.get(\"average_cost_for_two\", None),\n",
    "            document.get(\"currency\", \"\"),\n",
    "            document.get(\"has_table_booking\", \"\"),\n",
    "            document.get(\"has_online_delivery\", \"\"),\n",
    "            document.get(\"is_delivering_now\", \"\"),\n",
    "            document.get(\"switch_to_order_menu\", \"\"),\n",
    "            document.get(\"price_range\", None),\n",
    "            document.get(\"aggregate_rating\", None),\n",
    "            document.get(\"rating_color\", \"\"),\n",
    "            document.get(\"rating_text\", \"\"),\n",
    "            document.get(\"votes\", None)\n",
    "        ))\n",
    "\n",
    "        if len(lote) >= batch_size:\n",
    "            insertar_lote(lote)\n",
    "            lote = []\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error al procesar el documento con id {document_id}: {e}\")\n",
    "\n",
    "if lote:\n",
    "    insertar_lote(lote)\n",
    "\n",
    "# Cerrar las conexiones\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b8a3736-4b0f-4b32-b980-0497f19b0016",
   "metadata": {},
   "outputs": [],
   "source": [
    "couch = couchdb.Server(\"http://localhost:5984/\")\n",
    "usuario = \"admin\"\n",
    "contrasena = \"123456\"\n",
    "couch.resource.credentials = (usuario, contrasena)\n",
    "\n",
    "db = couch['autos_couchdb'] \n",
    "\n",
    "conn = pyodbc.connect(r'DRIVER={ODBC Driver 17 for SQL Server};'\n",
    "                      r'SERVER=DESKTOP-CPD7SBF\\SQLEXPRESS;'\n",
    "                      r'DATABASE=couchdb_sqlserver;'\n",
    "                      r'trusted_connection=yes')\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "IF NOT EXISTS (SELECT * FROM sysobjects WHERE name='electric_vehicles' AND xtype='U')\n",
    "CREATE TABLE electric_vehicles (\n",
    "    id NVARCHAR(50) PRIMARY KEY,\n",
    "    region NVARCHAR(100),\n",
    "    category NVARCHAR(100),\n",
    "    parameter NVARCHAR(100),\n",
    "    mode NVARCHAR(50),\n",
    "    powertrain NVARCHAR(50),\n",
    "    year INT,\n",
    "    unit NVARCHAR(50),\n",
    "    value FLOAT\n",
    ")\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "def insertar_lote(lote):\n",
    "    for fila in lote:\n",
    "        try:\n",
    "            cursor.execute(\"\"\"\n",
    "            INSERT INTO electric_vehicles (\n",
    "                id, region, category, parameter, mode, powertrain, year, unit, value\n",
    "            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\", fila)\n",
    "        except Exception as e:\n",
    "            print(f\"Error al insertar el registro con id {fila[0]}: {e}\")\n",
    "    conn.commit()\n",
    "\n",
    "batch_size = 7000\n",
    "lote = []\n",
    "\n",
    "for doc_id in db:\n",
    "    document = db[doc_id]\n",
    "    lote.append((\n",
    "        document.get(\"id\", None),\n",
    "        document.get(\"region\", None),\n",
    "        document.get(\"category\", None),\n",
    "        document.get(\"parameter\", None),\n",
    "        document.get(\"mode\", None),\n",
    "        document.get(\"powertrain\", None),\n",
    "        document.get(\"year\", None),\n",
    "        document.get(\"unit\", None),\n",
    "        document.get(\"value\", None)\n",
    "    ))\n",
    "\n",
    "    if len(lote) >= batch_size:\n",
    "        insertar_lote(lote)\n",
    "        lote = []\n",
    "\n",
    "if lote:\n",
    "    insertar_lote(lote)\n",
    "\n",
    "# Cerrar las conexiones\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f7b4a89-f072-4595-ad31-024a3136d3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------- ANALISIS DE SENTIMIENTOS ---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "08199263-ce8d-42a6-b601-62544b292fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentimiento_clasificado\n",
      "Neutral     4420\n",
      "Positivo    4143\n",
      "Negativo    2014\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# SENTIMIENTOS CSV BBC_NOTICIAS\n",
    "\n",
    "df = pd.read_csv('BBC_NOTICIAS.csv')\n",
    "\n",
    "def obtener_sentimiento(texto):\n",
    "    blob = TextBlob(texto)\n",
    "    return blob.sentiment.polarity\n",
    "\n",
    "def clasificar_sentimiento(polaridad):\n",
    "    if polaridad > 0:\n",
    "        return 'Positivo'\n",
    "    elif polaridad < 0:\n",
    "        return 'Negativo'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "df['sentimiento'] = df['description'].apply(obtener_sentimiento)\n",
    "df['sentimiento_clasificado'] = df['sentimiento'].apply(clasificar_sentimiento)\n",
    "\n",
    "conteo_sentimientos = df['sentimiento_clasificado'].value_counts()\n",
    "\n",
    "conteo_df = conteo_sentimientos.reset_index()\n",
    "conteo_df.columns = ['Sentimiento', 'Cantidad']\n",
    "\n",
    "conteo_df.to_csv('conteo_sentimientos_BBC_NOTICIAS.csv', index=False)\n",
    "\n",
    "print(conteo_sentimientos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ee514b0-221e-47fa-a540-5af7ff4537d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentimiento_clasificado\n",
      "Neutral     10542\n",
      "Positivo      591\n",
      "Negativo      234\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# SENTIMIENTOS CSV OLIMPICOS\n",
    "\n",
    "df = pd.read_csv('Olimpicos.csv')\n",
    "\n",
    "def obtener_sentimiento(texto):\n",
    "    blob = TextBlob(texto)\n",
    "    return blob.sentiment.polarity\n",
    "\n",
    "def clasificar_sentimiento(polaridad):\n",
    "    if polaridad > 0:\n",
    "        return 'Positivo'\n",
    "    elif polaridad < 0:\n",
    "        return 'Negativo'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "df['sentimiento'] = df['Event'].apply(obtener_sentimiento)\n",
    "df['sentimiento_clasificado'] = df['sentimiento'].apply(clasificar_sentimiento)\n",
    "\n",
    "conteo_sentimientos = df['sentimiento_clasificado'].value_counts()\n",
    "\n",
    "conteo_df = conteo_sentimientos.reset_index()\n",
    "conteo_df.columns = ['Sentimiento', 'Cantidad']\n",
    "\n",
    "conteo_df.to_csv('conteo_sentimientos_JuegosOlimpicos.csv', index=False)\n",
    "\n",
    "print(conteo_sentimientos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "be947321-ec0b-4c02-bd63-34c731f2f392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentimiento_clasificado\n",
      "Negativo    2464\n",
      "Positivo    2391\n",
      "Neutral     1263\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# SENTIMIENTOS CSV RESTAURANTES_RANKING\n",
    "\n",
    "df = pd.read_csv('Restaurantes_ranking.csv')\n",
    "\n",
    "def obtener_sentimiento(texto):\n",
    "    blob = TextBlob(texto)\n",
    "    return blob.sentiment.polarity\n",
    "\n",
    "def clasificar_sentimiento(polaridad):\n",
    "    if polaridad > 0:\n",
    "        return 'Positivo'\n",
    "    elif polaridad < 0:\n",
    "        return 'Negativo'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "df['sentimiento'] = df['Rating text'].apply(obtener_sentimiento)\n",
    "df['sentimiento_clasificado'] = df['sentimiento'].apply(clasificar_sentimiento)\n",
    "\n",
    "conteo_sentimientos = df['sentimiento_clasificado'].value_counts()\n",
    "\n",
    "conteo_df = conteo_sentimientos.reset_index()\n",
    "conteo_df.columns = ['Sentimiento', 'Cantidad']\n",
    "\n",
    "conteo_df.to_csv('conteo_sentimientos_Restaurantes.csv', index=False)\n",
    "\n",
    "print(conteo_sentimientos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ff75163f-75bd-4233-ab25-5da991310d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentimiento_clasificado\n",
      "Neutral     11735\n",
      "Positivo      918\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# SENTIMIENTOS CSV VENTAS Y POPULARIDAD AUTOS ELECTRICOS\n",
    "\n",
    "df = pd.read_csv('VentasYPopularidadDeAutosE.csv')\n",
    "\n",
    "def obtener_sentimiento(texto):\n",
    "    blob = TextBlob(texto)\n",
    "    return blob.sentiment.polarity\n",
    "\n",
    "def clasificar_sentimiento(polaridad):\n",
    "    if polaridad > 0:\n",
    "        return 'Positivo'\n",
    "    elif polaridad < 0:\n",
    "        return 'Negativo'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "df['sentimiento'] = df['powertrain'].apply(obtener_sentimiento)\n",
    "df['sentimiento_clasificado'] = df['sentimiento'].apply(clasificar_sentimiento)\n",
    "\n",
    "conteo_sentimientos = df['sentimiento_clasificado'].value_counts()\n",
    "\n",
    "conteo_df = conteo_sentimientos.reset_index()\n",
    "conteo_df.columns = ['Sentimiento', 'Cantidad']\n",
    "\n",
    "conteo_df.to_csv('conteo_sentimientos_Autos.csv', index=False)\n",
    "\n",
    "print(conteo_sentimientos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "55bae04e-b240-4164-bf7d-d2b4a46938d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentimiento_clasificado\n",
      "Muy Negativo    3331\n",
      "Negativo        1666\n",
      "Neutral         1333\n",
      "Muy Positivo     334\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# SENTIMIENTOS CSV PELI\n",
    "\n",
    "df = pd.read_csv('Peli.csv')\n",
    "\n",
    "def clasificar_sentimiento(popularity):\n",
    "    if popularity > 10000:\n",
    "        return 'Muy Positivo'\n",
    "    elif popularity > 5000:\n",
    "        return 'Positivo'\n",
    "    elif popularity > 1000:\n",
    "        return 'Neutral'\n",
    "    elif popularity > 100:\n",
    "        return 'Negativo'\n",
    "    else:\n",
    "        return 'Muy Negativo'\n",
    "        \n",
    "df['sentimiento_clasificado'] = df['popularity'].apply(clasificar_sentimiento)\n",
    "\n",
    "conteo_sentimientos = df['sentimiento_clasificado'].value_counts()\n",
    "\n",
    "conteo_df = conteo_sentimientos.reset_index()\n",
    "conteo_df.columns = ['Sentimiento', 'Cantidad']\n",
    "\n",
    "conteo_df.to_csv('conteo_sentimientos_Peliculas.csv', index=False)\n",
    "\n",
    "print(conteo_sentimientos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "097e6d61-d7ab-4f73-ac9c-5ac9a3398dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentimiento_clasificado\n",
      "Neutral    6664\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# SENTIMIENTOS CSV PELI FORMA 2\n",
    "\n",
    "df = pd.read_csv('Peli.csv')\n",
    "\n",
    "def obtener_sentimiento(texto):\n",
    "    blob = TextBlob(texto)\n",
    "    return blob.sentiment.polarity\n",
    "\n",
    "def clasificar_sentimiento(polaridad):\n",
    "    if polaridad > 0:\n",
    "        return 'Positivo'\n",
    "    elif polaridad < 0:\n",
    "        return 'Negativo'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "df['sentimiento'] = df['original_title'].apply(obtener_sentimiento)\n",
    "df['sentimiento_clasificado'] = df['sentimiento'].apply(clasificar_sentimiento)\n",
    "\n",
    "conteo_sentimientos = df['sentimiento_clasificado'].value_counts()\n",
    "\n",
    "conteo_df = conteo_sentimientos.reset_index()\n",
    "conteo_df.columns = ['Sentimiento', 'Cantidad']\n",
    "\n",
    "conteo_df.to_csv('conteo_n2_sentimientos_Peliculas.csv', index=False)\n",
    "\n",
    "print(conteo_sentimientos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7eb87d2-b7e7-4eec-aea5-1a5b465f8b2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
